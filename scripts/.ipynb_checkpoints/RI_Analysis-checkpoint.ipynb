{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import pyproj\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 1: Data Acquisition, Cleaning, and Outcome Indicator Creation\n",
    "\n",
    "### Note 1: The format of the NHC HURDAT2.csv file\n",
    "\n",
    "[From NHC] This dataset (known as Atlantic HURDAT2) has a comma-delimited, text format with six-hourly information on the location, maximum winds, central pressure, and (beginning in 2004) size of all known tropical cyclones and subtropical cyclones.\n",
    "\n",
    "The format of the HURDAT file looks like this:\n",
    "\n",
    "- **Storm Idenifier Row**\n",
    "- *Strom Observation Row*\n",
    "- *Strom Observation Row*\n",
    "- *Strom Observation Row*\n",
    "- **Storm Idenifier Row**\n",
    "- *Strom Observation Row*\n",
    "- *Strom Observation Row*\n",
    "- *Strom Observation Row*\n",
    "- *Strom Observation Row*\n",
    "\n",
    "#### Important!\n",
    "Within in an observation row, there is nothing that can identify which storm that the row belongs too.Thus, we need to add a storm identifier to each row for ease of use later. We do this in the `hurdat_lines_to_df` method below. \n",
    "\n",
    "See an example of the unchanged HURDAT data from Hurricane Irene below:\n",
    "\n",
    "`AL092011,  IRENE,  39,  \n",
    "20110821, 0000,  , TS, 15.0N,  59.0W,  45, 1006,  105, 0, 0,45, 0, 0, 0, 0, 0, 0, 0, 0,  \n",
    "20110821, 0600,  , TS, 16.0N,  60.6W,  45, 1006,  130, 0, 0,80, 0, 0, 0, 0, 0, 0, 0, 0,  \n",
    "20110821, 1200,  , TS, 16.8N,  62.2W,  45, 1005,  130, 0, 0,70, 0, 0, 0, 0, 0, 0, 0, 0,  \n",
    "20110821, 1800,  , TS, 17.5N,  63.7W,  50,  999,  130,20, 0,70,30, 0, 0, 0, 0, 0, 0, 0,  \n",
    "20110822, 0000,  , TS, 17.9N,  65.0W,  60,  993,  130,30,30,90,30, 0, 0,30, 0, 0, 0, 0,  \n",
    "20110822, 0600,  , HU, 18.2N,  65.9W,  65,  990,  130,60,60,90,40,25,20,35,25, 0, 0, 0,  \n",
    "20110822, 1200,  , HU, 18.9N,  67.0W,  70,  989,  160,60,60,90,40,25,20,35,25, 0, 0, 0,  \n",
    "20110822, 1800,  , HU, 19.3N,  68.0W,  75,  988,  160,60,40,90,40,30,20,35,25, 0, 0, 0, `\n",
    "\n",
    "The columns of the HURDAT file are as follows, refer to the link for more details.\n",
    "- Date (YYYYMMDD)\n",
    "- Time (24 hr)\n",
    "- Record Identifier (see link for more info)\n",
    "- Storm Status (Tropical Storm, Hurricane, etc. see link for more info)\n",
    "- Latitude\n",
    "- Longitude\n",
    "- Maximum Sustained Wind Speed (kts)\n",
    "- Minimum Pressure (mbar)\n",
    "- 34 kt wind radii maximum extent in NE quadrant (in nautical miles)\n",
    "- 34 kt wind radii maximum extent in SE quadrant (in nautical miles)\n",
    "- 34 kt wind radii maximum extent in SW quadrant (in nautical miles)\n",
    "- 34 kt wind radii maximum extent in NW quadrant (in nautical miles)\n",
    "- 50 kt wind radii maximum extent in NE quadrant (in nautical miles)\n",
    "- 50 kt wind radii maximum extent in SE quadrant (in nautical miles)\n",
    "- 50 kt wind radii maximum extent in SW quadrant (in nautical miles)\n",
    "- 50 kt wind radii maximum extent in NW quadrant (in nautical miles)\n",
    "- 64 kt wind radii maximum extent in NE quadrant (in nautical miles)\n",
    "- 64 kt wind radii maximum extent in SE quadrant (in nautical miles)\n",
    "- 64 kt wind radii maximum extent in SW quadrant (in nautical miles)\n",
    "- 64 kt wind radii maximum extent in NW quadrant (in nautical miles)\n",
    "\n",
    "To learn more about the format of the HURDAT2 file, see the description at https://www.nhc.noaa.gov/data/hurdat/hurdat2-format-nov2019.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Read in data from official HURDAT2.csv file\n",
    "\n",
    "Updated 10 June 2021 to include the 2020 season. These methods read in the raw HURDAT2 file and convert it into a lines object and then a dataframe. The method `hurdat_lines_to_df` adds the storm identifier \"Code\" to each row as described in Note 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hurdat_lines():\n",
    "    \"\"\"\n",
    "    read in official NHC HURDAT2.csv data file as line oject; \n",
    "    args: none; returns: line object\n",
    "    \"\"\"\n",
    "    f = open(\"../data/HURDAT2.csv\", \"r\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hurdat_lines_to_df(lines):\n",
    "    \"\"\"\n",
    "    convert HURDAT lines object to dataframe, accounting for necessary formating of HURDAT data file, see note (1);\n",
    "    args: lines object; returns: df with all storm observations, in tidy format(?)\n",
    "    \"\"\"\n",
    "    hurdat=[] # to store all observations as nested list\n",
    "    storm_info=[] # to store name and storm code\n",
    "    df = pd.DataFrame()\n",
    "    for line in lines:\n",
    "        arr = line.split(\",\")\n",
    "        # If this is a new storm, it will have \"AL\" in the first item ('AL' for Atlantic stroms)\n",
    "        # Since this is a new storm, we need to update storm info and not add this 'observation' to list\n",
    "        if \"AL\" in arr[0]: \n",
    "            storm_info = [arr[0],arr[1].strip()]\n",
    "        # If this is the same storm as previous row, add new observation to list\n",
    "        else:\n",
    "            arr.insert(0,storm_info[0])\n",
    "            arr.insert(1,storm_info[1])\n",
    "            hurdat.append(arr) \n",
    "    df = pd.DataFrame(hurdat)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Prettify the Dataframe (Rename, Strip, Retype)\n",
    "\n",
    "Make the dataframe easier to work with by cleaning it up in several different ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df):\n",
    "    \"\"\" rename columns from HURDAT file, see note (1); returns given dataframe \"\"\"\n",
    "    # note we are leaving the storm radii columns (index 10-21) alone for now, we deal with those later\n",
    "    col_names = {\n",
    "        df.columns[0]: 'Code',df.columns[1]: 'Name',  df.columns[2]: 'Date',\n",
    "        df.columns[3]: 'Time', df.columns[4]: 'Record',df.columns[5]: 'Status',\n",
    "        df.columns[6]: 'Lat',  df.columns[7]: 'Lon',   df.columns[8]: 'Wind',\n",
    "        df.columns[9]: 'Pressure'\n",
    "    }\n",
    "    df = df.rename(columns = col_names) # rename columns according to dictionary\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_string_columns(df):\n",
    "    \"\"\" Strip extra spaces on all object columns from raw HURDAT file; returns given dataframe \"\"\"\n",
    "    df_obj = df.select_dtypes(['object'])\n",
    "    df[df_obj.columns] = df_obj.apply(lambda x: x.str.strip()) # strip spaces from string columns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retype_columns(df):\n",
    "    \"\"\" convert column types into something more useful; returns given dataframe \"\"\"\n",
    "    df[['Wind','Pressure']] = df[['Wind','Pressure']].astype(str).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HURDAT stores Date and Time seperately, but combining them into one datetime column makes life soo much better ðŸ˜œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datetime_column(df):\n",
    "    \"\"\"\n",
    "    HURDAT data has date and time in seperate columns, but we combine into one DateTime;\n",
    "    removes seperate date and time columns; \n",
    "    returns given dataframe\n",
    "    \"\"\"\n",
    "    df[\"DateTime\"] = df[\"Date\"] + ' ' + df[\"Time\"] # combine string columns\n",
    "    df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"], format = '%Y%m%d %H%M') # convert to datetime object\n",
    "    df = df.drop(columns=['Date', 'Time'], axis=1) # remove unneeded columns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Convert coordinates from string (19.7W) to float (-19.7)\n",
    "\n",
    "R (and others) understands coordinates better as numbers instead of text, so we convert the string coordinates to numerics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_coordinates(df):\n",
    "    \"\"\"\n",
    "    convert HURDAT string coordinates to something that visualizations can understand (float between -180 and 180); \n",
    "    string coords have number and direction, we split and multiply by -1 depending on hemisphere;\n",
    "    returns given dataframe\n",
    "    \"\"\"\n",
    "    for direc in ['Lat','Lon']: # loop for both coordinates\n",
    "        df[f'{direc}_Hemisphere'] = df[f'{direc}'].str[-1:] # get the direction string (N,E,S,W)\n",
    "        df[f'{direc}'] = df[f'{direc}'].str[:-1].astype(float) # get numeric value\n",
    "        # function to multiply value depending on direction string\n",
    "        convert_direc = lambda row: row[f\"{direc}\"]*-1 if row[f\"{direc}_Hemisphere\"] in ['S','W'] else row[f\"{direc}\"]\n",
    "        df[f'{direc}'] = df.apply(convert_direc, axis=1) # apply lambda func to get final readable coordinate\n",
    "        df = df.drop(columns=[f\"{direc}_Hemisphere\"]) # remove unneeded column\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Create `Rapid_NHC{k}` columns\n",
    "\n",
    "These columns denote whether a storm was Rapid Increasing (defined as an increase of 30 knots in Maximum Sustained Windspeed over the previous k hours) at the time of the observation.\n",
    "\n",
    "The NHC defines RI as an increae of 30kts in wind speed over 24 hours (i.e. k=24). However, there is some debate in the literature over the arbitary nature of this definition, hence we include other RI indiciators as well. Specifically, whether the storm increased by 30kts in the last k hours, where k=6,12,18,24,30,36. \n",
    "\n",
    "We chose to split on 6 hour intervals because the HURDAT data comes on 6 hours intervals. Obviously, these are not equivalent metrics, a storm that increases by 30kts in 6 hours is on a rampage compared to one that does the same in the 6-times-as-long 36 hour window. We choose the use these different metrics because the ML model may have a better time predicting one compared to the other, but we shall see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def within_k_hours_before(time, series, k):\n",
    "    \"\"\"\n",
    "    get items in series whose datetime is within k hours of argument time;\n",
    "    args: time (datetime), series (series) of HURDAT observations;\n",
    "    return: series with items whose datetime is within k hours of the argument time\n",
    "    \"\"\"\n",
    "    return (time-timedelta(hours=k) <= series) & (series <= time)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_k_rapid_NHC_column(df, K):\n",
    "    \"\"\"\n",
    "    exactly the same as 'create_rapid_NHC_column' but with variable amount of hours instead of only 24; \n",
    "    create boolean columns if storm was 'Rapidly Increasing' per modified NHC defn. at the time of the observation;\n",
    "    we define modified defn. as RI if there is an increase of 30+ knots in k hours;\n",
    "    args: df of storm observations, K (list) int list to create modified RI columns for each k hours in K\n",
    "    \"\"\"\n",
    "    storm_codes = df.Code.unique() # list of unique stroms\n",
    "    for index_storm, storm in enumerate(storm_codes): # iterate through all stroms \n",
    "        df_storm = df[df.Code == storm] # get df for each storm\n",
    "        for k in K: # iterate through multiple new modified RI columns\n",
    "            for index_row, row in df_storm.iterrows():\n",
    "                # get current time and wind for the row\n",
    "                current_time = row['DateTime']; \n",
    "                current_wind = row[\"Wind\"] \n",
    "                \n",
    "                # find all observations within k hours of current observation\n",
    "                df_k_hours_before = df_storm[ within_k_hours_before(current_time, df_storm['DateTime'], k) ]\n",
    "                \n",
    "                # get minimum wind within k hours\n",
    "                min_wind_k_hours = df_k_hours_before[\"Wind\"].min() \n",
    "\n",
    "                # determine if strom was RI if it increased by 30mph in the last k hours\n",
    "                df.loc[index_row,f\"Rapid_NHC{k}\"] = (current_wind - min_wind_k_hours ) > 30 \n",
    "          \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_k_rapid_NHC_column_outcome(df, K):\n",
    "    storm_codes = df.Code.unique() # list of unique stroms\n",
    "    for storm in storm_codes: # iterate through all stroms \n",
    "        df_storm = df[df.Code == storm] # get df for each storm\n",
    "        for k in K:\n",
    "            for index_row, row in df_storm.iterrows():\n",
    "                current_time = row['DateTime']; \n",
    "                df_k_hours_after = (df_storm[ within_k_hours_after(current_time, df_storm['DateTime'], k) ])\n",
    "                df.loc[index_row,f\"Rapid_NHC{k}_Outcome\"] = True in df_k_hours_after[f'Rapid_NHC{k}'].tolist()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def within_k_hours_after(time, series, k):\n",
    "    \"\"\"\n",
    "    get items in series whose datetime is within k hours of argument time;\n",
    "    args: time (datetime), series (series) of HURDAT observations;\n",
    "    return: series with items whose datetime is within k hours of the argument time\n",
    "    \"\"\"\n",
    "    return (time <= series) & (series <= time+timedelta(hours=k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Wind Speed Acceleration\n",
    "\n",
    "Similar to the `Rapid_NHC{k}` columns, the `Acceleration` column gives the average acceleration of wind speed (in kts/hr) since the last observation. \n",
    "\n",
    "For example: If Storm X has a wind speed of 100 kts at 1200 and increased to 120 kts at 1800, it would have an average acceleration of 20/6 = 3.333 kts/hr.\n",
    "\n",
    "Note: This calculation is made difficult by the sometimes inconsistent interval between observations. Observations usually come every 6 hours, but if the storm makes landfall inbetween, there will be a newobservation at that time. In theory I could remove observations not on the 6-hour interval but I don't want to do that.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_in_hours(dt1,dt2):\n",
    "    \"\"\"\n",
    "    calculate the difference between two datetimes in hours, copied from stack; \n",
    "    args: dt1 (datetime), dt2 (datetime); returns: hours between dt1 and dt2;\n",
    "    \"\"\"\n",
    "    difference = dt1 - dt2 # get difference in datetime format\n",
    "    days, seconds = difference.days, difference.seconds # extract days and seconds bc hours is not native(??)\n",
    "    hours = days * 24 + seconds // 3600 # calculate hours\n",
    "    return hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_acceleration(delta_wind, delta_time):\n",
    "    \"\"\"\n",
    "    calculate the acceleration per hour for a single strom interval;\n",
    "    args: \n",
    "        delta_wind (float): the change in wind in an interval\n",
    "        delta_time (float): the length in hours of the interval\n",
    "    returns: acceleration per hour (float) of wind during the interval\n",
    "    \"\"\"\n",
    "    try:\n",
    "        accel = delta_wind/delta_time\n",
    "    except:\n",
    "        # if delta_time = 0, let acceleration = 0\n",
    "        accel = 0\n",
    "    # 25 mph change in one hour is far too high to be plausible, so return 0 because something went wrong\n",
    "    return accel if abs(accel) < 25 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_acceleration_column(df):\n",
    "    \"\"\"\n",
    "    storm observations usually come every 6 hours, though not always. this method calculates the change in \n",
    "    acceleration since the last observation for that storm.\n",
    "    returns the given dataframe, with a new column for accleration\n",
    "    \"\"\"\n",
    "    df[f\"Accel\"] = 0 # set default acceleration to 0\n",
    "    storm_codes = df.Code.unique() # get all unique storms\n",
    "    # iterate through each storm code, create a dataframe for each strom\n",
    "    for index_storm, storm in enumerate(storm_codes):\n",
    "        df_storm = df[df.Code == storm] # get strom specific df\n",
    "        current_wind = previous_wind = 0 # set default values to 0\n",
    "        first_index = np.inf; # reset index to high number\n",
    "        # loop through each row of the storm df\n",
    "        for index_row, row in df_storm.iterrows():\n",
    "            # record index of first row in storm df, to get assigned 0 acceleration later\n",
    "            if index_row < first_index: first_index = index_row\n",
    "            try:\n",
    "                # get current weather values\n",
    "                current_wind = df.iloc[index_row]['Wind']\n",
    "                current_time = df.iloc[index_row]['DateTime']\n",
    "                \n",
    "                # get weather values for previous row\n",
    "                previous_wind = df.iloc[index_row-1]['Wind']\n",
    "                previous_time = df.iloc[index_row-1]['DateTime']\n",
    "                \n",
    "                # calculate change in time and wind, used to calculate accleration\n",
    "                delta_wind = current_wind - previous_wind\n",
    "                delta_time = difference_in_hours(current_time,previous_time)\n",
    "                acceleration = calculate_acceleration(delta_wind,delta_time)\n",
    "            except:\n",
    "                acceleration = 0 # if any of the above failed, then set acceleration to 0\n",
    "            df.loc[index_row,'Accel'] = acceleration # set acceleration in full dataframe\n",
    "        df.loc[first_index,'Accel'] = 0 # set acceleration of first row in storm df to 0\n",
    "        first_index = np.inf; # reset index of first row\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) Calculate Radii of High Intensity\n",
    "\n",
    "The last set of columns (the ones we ignored earlier) described the 'size' of the storm. Specifically, they give the maximum extent (in nautical miles) of a certain wind speed in one quadrant. Then, we average these four quadrants together to get the average maximum extent of the three wind speeds, 34kt, 50kt, and 64 kts. This tells us the average radius of the storm with wind speeds above these marks, giving us three proxies for 'storm size'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wind_radii(df):\n",
    "    \"\"\"\n",
    "    calculate the average radius of different wind speed by average the wind extent in the four quadrants;\n",
    "    HURDAT data has extent of 34,50,64kt winds in 4 quadrants (NW,SW,SE,NE), we average them to just 3 numbers;\n",
    "    returns given dataframe, with 3 new columns for extent of 34,50,64kt winds\n",
    "    \"\"\"\n",
    "    # get average of 4 quadrant observations for 3 wind categories, and record as single column\n",
    "    df[\"34kt_radius\"] = df[[10,11,12,13]].astype(str).astype(int).mean(axis=1) \n",
    "    df[\"50kt_radius\"] = df[[14,15,16,17]].astype(str).astype(int).mean(axis=1)\n",
    "    df[\"64kt_radius\"] = df[[18,19,20,21]].astype(str).astype(int).mean(axis=1)\n",
    "    df.drop(columns = [10,11,12,13,14,15,16,17,18,19,20,21,22], inplace=True) # drop unneeded columns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g) Calculate Bearing, Distance, and Storm Speed\n",
    "\n",
    "Create additional columns that may be useful to an ML model, including:\n",
    "- Bearing: the bearing (direction) the storm is moving in (between -180 and 180)\n",
    "- Distance: the distance the storm has traveled since the last observation in nautical miles\n",
    "- Speed: the speed of the storm track (not wind speed) in knots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bearing_column(df):\n",
    "    geodesic = pyproj.Geod(ellps='WGS84')\n",
    "    df[f\"Bearing\"] = 0 # set default bearing to 0\n",
    "    df[f\"Distance\"] = 0 # set default bearing to 0\n",
    "    df[f\"Speed\"] = 0 # set default bearing to 0\n",
    "    storm_codes = df.Code.unique() # get all unique storms\n",
    "    # iterate through each storm code, create a dataframe for each strom\n",
    "    for index_storm, storm in enumerate(storm_codes):\n",
    "        df_storm = df[df.Code == storm] # get strom specific df\n",
    "        current_bearing = previous_bearing = 0 # set default values to 0\n",
    "        first_index = np.inf; # reset index to high number\n",
    "        \n",
    "        # loop through each row of the storm df\n",
    "        for index_row, row in df_storm.iterrows():\n",
    "            \n",
    "            # record index of first row in storm df, to get assigned 0 later\n",
    "            if index_row < first_index: \n",
    "                first_index = index_row\n",
    "                \n",
    "            try:\n",
    "                # calculate change in time since last obersvation\n",
    "                current_time = df.iloc[index_row]['DateTime']\n",
    "                previous_time = df.iloc[index_row-1]['DateTime']\n",
    "                delta_time = difference_in_hours(current_time,previous_time)\n",
    "                \n",
    "                # get current position\n",
    "                current_position = [df.iloc[index_row]['Lat'],df.iloc[index_row]['Lon']]\n",
    "                \n",
    "                # get previous position\n",
    "                previous_position = [df.iloc[index_row-1]['Lat'],df.iloc[index_row-1]['Lon']]\n",
    "                \n",
    "                # use geodesic to calculate bearing and distance between observations\n",
    "                bearing, back_bearing, distance = geodesic.inv(\n",
    "                    previous_position[1], previous_position[0], current_position[1], current_position[0])\n",
    "                \n",
    "                # calculate distance and speed in nautical miles\n",
    "                distance = distance/1852 # 1 nautical mile = 1852 meters\n",
    "                speed = distance / delta_time\n",
    "            except:\n",
    "                # if any error occured, let all values equal 0\n",
    "                bearing = 0; distance = 0; speed = 0;\n",
    "                \n",
    "            # set the values for the current row\n",
    "            df.loc[index_row,'Bearing'] = bearing\n",
    "            df.loc[index_row,'Distance'] = distance\n",
    "            df.loc[index_row,'Speed'] = speed\n",
    "            \n",
    "        # set the values for the first row of storm\n",
    "        df.loc[first_index,'Bearing'] = 0\n",
    "        df.loc[first_index,'Distance'] = 0\n",
    "        df.loc[first_index,'Speed'] = 0\n",
    "        \n",
    "        # reset index of first row\n",
    "        first_index = np.inf; \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h) Remove storms that do not reach Category X\n",
    "\n",
    "Not all storms in HURDAT dataset are hurricanes, there are also distrubrances, waves, tropical storms and more. In fact, more than half of the storms do not reach the 64kt barrier to be classified as a category 1 hurricane. Beyond that, only 1/3 of all hurricanes are considered 'major', making it to category 3,4 or 5.\n",
    "\n",
    "These methods helps to determine the category of the hurricane based on its wind speed. Then, we can remove storms that do not reach the desired category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_category(wind):\n",
    "    \"\"\"\n",
    "    args: wind speed; returns: storm category (0 to 5) based on the given wind speed\n",
    "    \"\"\"\n",
    "    cat_breaks = [64,83,96,113,137,300]\n",
    "    for i,cat in enumerate(cat_breaks):\n",
    "        if wind > 0 and wind < cat:\n",
    "            return i\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_storms_below_cat_X(df, X):\n",
    "    \"\"\"\n",
    "    remove storms from the full dataset if the storm did not reach category X;\n",
    "    args: df: full strom df, X (int): remove stroms if they do not meet category X\n",
    "    \"\"\"\n",
    "    if X not in range(1,6): return df # if not a valid storm category, retrun the full df\n",
    "    # minimum wind speed for each category hurricane\n",
    "    cat_X_minimum_wind_speeds = {1:64 , 2:83, 3:96, 4:113, 5:137} \n",
    "    # compare max wind speed of strom with desired category miniumum, remove storm if it does not reach threshold\n",
    "    df = df[(df.groupby('Code')['Wind'].transform('max')) >= cat_X_minimum_wind_speeds[X]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read in data\n",
    "lines = read_hurdat_lines()\n",
    "df = hurdat_lines_to_df(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up dataframe\n",
    "df = rename_columns(df)\n",
    "df = strip_string_columns(df)\n",
    "df = create_datetime_column(df)\n",
    "df = retype_columns(df)\n",
    "\n",
    "# Create new columns from calculations\n",
    "df = convert_coordinates(df)\n",
    "df = calculate_wind_radii(df)\n",
    "df = create_bearing_column(df)\n",
    "df = create_k_rapid_NHC_column(df,[12,24])\n",
    "df = create_k_rapid_NHC_column_outcome(df, [12,24])\n",
    "df = create_acceleration_column(df)\n",
    "\n",
    "# Remove unneeded stroms\n",
    "# df = remove_storms_below_cat_X(df,1)\n",
    "# df = df.reset_index(drop=True)\n",
    "\n",
    "# Create Pickle\n",
    "d = datetime.today()\n",
    "df.to_pickle(f\"{d.month}_{d.day}_{d.year}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 999\n",
    "df[df.Code == 'AL092017']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = datetime.today()\n",
    "df.to_pickle(f\"{d.month}_{d.day}_{d.year}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read Pickle\n",
    "df = pd.read_pickle(\"12_7_2021.pkl\")\n",
    "df[df.Code == 'AL292020']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to .txt file for R visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storm_codes = df.Code.unique()\n",
    "f = open('Storm_Track_Segments.csv',\"w\")\n",
    "f.write(f\"Code,Name,Year,Month,Hour,Lat1,Lon1,Lat2,Lon2,Wind,Pressure,Accel,Bearing,Speed,Distance,Rapid_NHC24,Rapid_NHC18,Rapid_NHC12,Rapid_NHC6,kt34,kt50,kt64,\\n\")\n",
    "for storm_index, storm in enumerate(storm_codes):\n",
    "    # get_progress(storm_index,len(storm_codes),10) \n",
    "    df_storm = df[df.Code == storm ]\n",
    "    for i in range(len(df_storm)-1):\n",
    "        Code = df_storm.iloc[i,:].Code\n",
    "        Name = df_storm.iloc[i,:].Name\n",
    "        Year = df_storm.iloc[i,:].DateTime.year\n",
    "        Month = df_storm.iloc[i,:].DateTime.month\n",
    "        Hour = df_storm.iloc[i,:].DateTime.hour\n",
    "        Lat1 = df_storm.iloc[i,:].Lat\n",
    "        Lon1 = df_storm.iloc[i,:].Lon\n",
    "        Lat2 = df_storm.iloc[i+1,:].Lat\n",
    "        Lon2 = df_storm.iloc[i+1,:].Lon\n",
    "        Wind = df_storm.iloc[i,:].Wind\n",
    "        Pressure = df_storm.iloc[i,:].Pressure\n",
    "        Accel = df_storm.iloc[i,:].Accel\n",
    "        Bearing = df_storm.iloc[i,:].Bearing\n",
    "        Speed = df_storm.iloc[i,:].Speed\n",
    "        Distance = df_storm.iloc[i,:].Distance\n",
    "        Rapid_NHC24 = df_storm.iloc[i,:].Rapid_NHC24\n",
    "        Rapid_NHC18 = df_storm.iloc[i,:].Rapid_NHC18\n",
    "        Rapid_NHC12 = df_storm.iloc[i,:].Rapid_NHC12\n",
    "        Rapid_NHC6 = df_storm.iloc[i,:].Rapid_NHC6\n",
    "        kt34 = df_storm.iloc[i,:]['34kt_radius']\n",
    "        kt50 = df_storm.iloc[i,:]['50kt_radius']\n",
    "        kt64 = df_storm.iloc[i,:]['64kt_radius']\n",
    "        f.write(f\"{Code},{Name},{Year},{Month},{Hour},{Lat1},{Lon1},{Lat2},{Lon2},{Wind},{Pressure},{Accel},{Bearing},{Speed},{Distance},{Rapid_NHC24},{Rapid_NHC18},{Rapid_NHC12},{Rapid_NHC6},{kt34},{kt50},{kt64},\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Create a Strom Dataframe\n",
    "\n",
    "Now we transition our unit of analysis from an observation of a storm to the storm itself. This new dataframe will have one row per storm from the HURDAT dataset. Changing the unit of analysis will allow us to understand larger characteristics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_all(df):\n",
    "    df_all = pd.DataFrame()\n",
    "    codes = df.Code.unique()\n",
    "    for code in codes:\n",
    "        df_strom = df[df.Code == code]\n",
    "        df_all = df_all.append({\n",
    "            'Code'         : code,\n",
    "            'Name'         : df_strom.iloc[0].Name,\n",
    "            'Start_Date'   : df_strom.DateTime.min(),\n",
    "            'End_Date'     : df_strom.DateTime.max(),\n",
    "            'Min_Lat'      : df_strom.Lat.max(),\n",
    "            'Max_Lat'      : df_strom.Lat.max(),\n",
    "            'Min_Lon'      : df_strom.Lon.max(),\n",
    "            'Max_Lon'      : df_strom.Lon.max(),\n",
    "            'Max_Wind'     : df_strom.Wind.max(),\n",
    "            'Min_Pressure' : df_strom.Pressure.max(),\n",
    "            'Max_Accel'    : df_strom.Accel.max(),\n",
    "            'Category'     : calculate_category(df_strom.Wind.max()),\n",
    "            'Max_34kt'     : df_strom['34kt_radius'].max(),\n",
    "            'Max_50kt'     : df_strom['50kt_radius'].max(),\n",
    "            'Max_64kt'     : df_strom['64kt_radius'].max(),\n",
    "            'Rapid_NHC6'   : df_strom.Rapid_NHC6.any(),\n",
    "            'Rapid_NHC12'  : df_strom.Rapid_NHC12.any(),\n",
    "            'Rapid_NHC18'  : df_strom.Rapid_NHC18.any(),\n",
    "            'Rapid_NHC24'  : df_strom.Rapid_NHC24.any(),\n",
    "        }, ignore_index=True)\n",
    "        df_all.Category = df_all.Category.astype(int)\n",
    "    return df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"11_27_2021.pkl\")\n",
    "df_all = create_df_all(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can learn about characteristics of the all the recorded storms since 1850!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all.Rapid_NHC24.sum() / len(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since records began in 1850, we find that 17% of recorded storms would have been classified as 'Rapidly Increasing' according to the NHC definition.\n",
    "\n",
    "Let's see how that has changed over time. First, let's create a custom rounding function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_round(x, base=1):\n",
    "    \"\"\" \n",
    "    custom rounding function that rounds down to the nearest base;\n",
    "    args: \n",
    "        x (float): number to round\n",
    "        base (float): base to round to\n",
    "    returns:\n",
    "        num (int): x rounded down the the nearest base\n",
    "    \"\"\"\n",
    "    return int(base * math.floor(float(x)/base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we calculate the Year and Decade that each storm occurred in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Year'] = pd.DatetimeIndex(df_all['Start_Date']).year\n",
    "df_all['Decade'] = pd.DatetimeIndex(df_all['Start_Date']).year.astype(float)\n",
    "df_all['Decade'] = df_all['Decade'].apply(lambda x: custom_round(x, base=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.set(rc = {'figure.figsize':(20,7)})\n",
    "\n",
    "fig, ax = plt.subplots();\n",
    "\n",
    "df2 = df_all.groupby(['Decade','Rapid_NHC24'])['Decade'].count().unstack('Rapid_NHC24').fillna(0)\n",
    "\n",
    "colors = ['#aaaaaa', '#EACE09']\n",
    "bottom = np.zeros(len(df2))\n",
    "\n",
    "for i, col in enumerate(df2.columns):\n",
    "  ax.bar(df2.index, df2[col], bottom=bottom, label=col, color=colors[i], width=10)\n",
    "  bottom += np.array(df2[col])\n",
    "\n",
    "# Sum up the rows of our data to get the total value of each bar.\n",
    "totals = df2.sum(axis=1)\n",
    "percs = df2[1.0]/(df2[1.0]+df2[0.0])\n",
    "\n",
    "# Add labels to each bar.\n",
    "for i, total in enumerate(totals):\n",
    "  ax.text(totals.index[i], \n",
    "          df2.iloc[i][0.0] + .5*df2.iloc[i][1.0] - 2.7, \n",
    "          f\"{int(100*round(percs.iloc[i],2))}%\", \n",
    "          ha='center', weight='bold', fontsize=13)\n",
    "\n",
    "plt.xticks(df2.index.to_numpy())\n",
    "ax.set_xticklabels([ f\"{d}s\" for d in df2.index.to_numpy()])\n",
    "\n",
    "plt.xlabel('Decade',fontsize=20,labelpad=10)\n",
    "plt.ylabel('Storm Count',fontsize=20,labelpad=10)\n",
    "\n",
    "ax.tick_params(axis='both', labelsize=14 )\n",
    "\n",
    "ax.set_title('Total and Rapidly Increasing Storms by Decade (1/2)', fontsize=24, pad=20)\n",
    "ax.legend(labels=('No RI','RI'), prop={'size': 16});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.metrics as metrics\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "sns.set(rc = {'figure.figsize':(20,7)})\n",
    "\n",
    "df2 = df_all.groupby(['Decade','Rapid_NHC24'])['Decade'].count().unstack('Rapid_NHC24').fillna(0)[:-1]\n",
    "X = df2.index.to_numpy()\n",
    "y_rapid = (df2[1.0]/(df2[1.0]+df2[0.0])).to_numpy()\n",
    "y_all = df2.sum(axis=1).to_numpy()\n",
    "\n",
    "X = df2.index.to_numpy().reshape(-1, 1)\n",
    "\n",
    "reg_rapid = LinearRegression().fit(X, y_rapid)\n",
    "reg_all = LinearRegression().fit(X, y_all)\n",
    "\n",
    "fig, ax_left = plt.subplots()\n",
    "ax_right = ax_left.twinx()\n",
    "\n",
    "ax_left.set_ylim([0, 300])\n",
    "ax_left.set_xlim([1843, 2017])\n",
    "ax_right.set_ylim([0,0.30])\n",
    "\n",
    "plt.xticks(np.arange(1850, 2020, step=10))\n",
    "ax_left.set_xticklabels([ f\"{d}s\" for d in df2.index.to_numpy()])\n",
    "\n",
    "ax_left.set_ylabel('Storm Count',fontsize=20,labelpad=10)\n",
    "ax_right.set_ylabel('Percent Rapidly Increasing',fontsize=20,labelpad=10)\n",
    "ax_left.set_xlabel('Decade',fontsize=20,labelpad=10)\n",
    "\n",
    "ax_left.tick_params(axis='y', labelsize=14 )\n",
    "ax_right.tick_params(axis='y', labelsize=14 )\n",
    "ax_left.tick_params(axis='x', labelsize=14 )\n",
    "\n",
    "ax_left.set_title('Total and Rapidly Increasing Storms by Decade (2/2)', fontsize=24, pad=20)\n",
    "\n",
    "ax_left.plot(X, y_all, 'o', color='#aaaaaa', markersize=10)\n",
    "ax_right.plot(X, y_rapid, 'o', color='#EACE09', markersize=10)\n",
    "ax_left.plot(X, reg_all.predict(X), color='#aaaaaa', linewidth=3)\n",
    "ax_right.plot(X, reg_rapid.predict(X), color='#EACE09', linewidth=3)\n",
    "\n",
    "plt.axvline(x=1843.8, linewidth=6, color='#aaaaaa');\n",
    "plt.axvline(x=2016.4, linewidth=6, color='#EACE09');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "sns.set()\n",
    "sns.set(rc = {'figure.figsize':(20,8)})\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "df2 = df_all.groupby(['Decade','Category'])['Decade'].count().unstack('Category').fillna(0)\n",
    "df2 = df2.drop([2020],axis=0)\n",
    "\n",
    "df2 = df_all.groupby(['Decade','Category'])['Decade'].count().unstack('Category').fillna(0)\n",
    "df2 = df2.drop([0], axis=1)\n",
    "df2 = df2.div(df2.sum(axis=1), axis=0)\n",
    "colors = ['#F64A4A','#F06E6E','#EB9292','#E5B6B6','#E0DBDB']\n",
    "bottom = np.zeros(len(df2))\n",
    "\n",
    "for i, col in enumerate(df2.columns):\n",
    "  ax.bar(df2.index, df2[col], bottom=bottom, label=col, color=colors[4-i], width=10)\n",
    "  bottom += np.array(df2[col])\n",
    "\n",
    "plt.xticks(np.arange(1850, 2030, step=10))\n",
    "ax.set_xticklabels([ f\"{d}s\" for d in df2.index.to_numpy()])\n",
    "\n",
    "ax.tick_params(axis='both', labelsize=14, labelright=True )\n",
    "ax.set_xlim([1845, 2025])\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "ax.set_ylabel('Proportion by Decade',fontsize=20,labelpad=10)\n",
    "ax.set_xlabel('Decade',fontsize=20,labelpad=10)\n",
    "\n",
    "ax.set_title('Hurricane Category Occurences by Decade', fontsize=24, pad=20)\n",
    "ax.legend(labels=('Category 1','Category 2','Category 3','Category 4','Category 5'), prop={'size': 16});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HURDAT Record Identifiers\n",
    "\n",
    "These values provide additional information about a specific observation from a storm. The one we are most interested in is the 'L' identifier for Landfall.\n",
    "\n",
    "While prediciting RI events anywhere is useful in theory, **we are most concerned with predicting RI when the storm is close to land.**\n",
    "\n",
    "C â€“ Closest approach to a coast, not followed by a landfall  \n",
    "G â€“ Genesis  \n",
    "I â€“ An intensity peak in terms of both pressure and wind  \n",
    "L â€“ Landfall (center of system crossing a coastline)  \n",
    "P â€“ Minimum in central pressure  \n",
    "R â€“ Provides additional detail on the intensity of the cyclone when rapid changes are underway  \n",
    "S â€“ Change of status of the system  \n",
    "T â€“ Provides additional detail on the track (position) of the cyclone     \n",
    "W â€“ Maximum sustained wind spee"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
